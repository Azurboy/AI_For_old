import React, { useEffect, useRef, useState } from 'react'
import './VoiceActivityDetector.css'

/**
 * F-004: è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)
 * è‡ªåŠ¨æ£€æµ‹ç”¨æˆ·è¯´è¯ï¼Œæ— éœ€"æŒ‰ä½è¯´è¯"
 */
function VoiceActivityDetector({ onSpeechDetected, isEnabled }) {
  const [isListening, setIsListening] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const mediaRecorderRef = useRef(null)
  const audioContextRef = useRef(null)
  const analyserRef = useRef(null)
  const silenceTimerRef = useRef(null)
  const recordingChunksRef = useRef([])

  useEffect(() => {
    if (!isEnabled) {
      stopListening()
      return
    }

    startListening()

    return () => {
      stopListening()
    }
  }, [isEnabled])

  const startListening = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      })

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)()
      analyserRef.current = audioContextRef.current.createAnalyser()
      const source = audioContextRef.current.createMediaStreamSource(stream)
      source.connect(analyserRef.current)
      analyserRef.current.fftSize = 2048

      // åˆ›å»ºMediaRecorder
      mediaRecorderRef.current = new MediaRecorder(stream)
      
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          recordingChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(recordingChunksRef.current, { type: 'audio/webm' })
        recordingChunksRef.current = []
        
        // åªå‘é€å¤§äº1ç§’çš„å½•éŸ³
        if (audioBlob.size > 10000) {
          onSpeechDetected(audioBlob)
        }
      }

      setIsListening(true)
      detectVoiceActivity()

    } catch (error) {
      console.error('æ— æ³•è®¿é—®éº¦å…‹é£:', error)
      alert('è¯·å…è®¸è®¿é—®éº¦å…‹é£ä»¥ä½¿ç”¨è¯­éŸ³åŠŸèƒ½')
    }
  }

  const stopListening = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }

    if (audioContextRef.current) {
      audioContextRef.current.close()
    }

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
    }

    setIsListening(false)
    setIsRecording(false)
  }

  const detectVoiceActivity = () => {
    if (!analyserRef.current) return

    const bufferLength = analyserRef.current.fftSize
    const dataArray = new Uint8Array(bufferLength)

    const checkAudio = () => {
      if (!isEnabled || !analyserRef.current) return

      analyserRef.current.getByteTimeDomainData(dataArray)

      // è®¡ç®—éŸ³é‡
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        const value = (dataArray[i] - 128) / 128.0
        sum += value * value
      }
      const volume = Math.sqrt(sum / bufferLength)

      // VADé˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰
      const VOICE_THRESHOLD = 0.02
      const SILENCE_DURATION = 1500 // 1.5ç§’æ— å£°ååœæ­¢å½•éŸ³

      if (volume > VOICE_THRESHOLD) {
        // æ£€æµ‹åˆ°å£°éŸ³
        if (!isRecording) {
          console.log('ğŸ¤ å¼€å§‹å½•éŸ³')
          setIsRecording(true)
          recordingChunksRef.current = []
          mediaRecorderRef.current.start()
        }

        // æ¸…é™¤é™éŸ³è®¡æ—¶å™¨
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current)
        }

        // è®¾ç½®æ–°çš„é™éŸ³è®¡æ—¶å™¨
        silenceTimerRef.current = setTimeout(() => {
          if (isRecording && mediaRecorderRef.current.state === 'recording') {
            console.log('â¸ï¸ åœæ­¢å½•éŸ³ï¼ˆé™éŸ³ï¼‰')
            mediaRecorderRef.current.stop()
            setIsRecording(false)
            
            // é‡æ–°å¼€å§‹ç›‘å¬
            setTimeout(() => {
              if (mediaRecorderRef.current && isEnabled) {
                mediaRecorderRef.current.start()
              }
            }, 100)
          }
        }, SILENCE_DURATION)
      }

      requestAnimationFrame(checkAudio)
    }

    checkAudio()
  }

  return (
    <div className="vad-indicator">
      {isListening && (
        <>
          <div className={`vad-status ${isRecording ? 'recording' : 'listening'}`}>
            <div className="vad-dot" />
            <span>{isRecording ? 'æ­£åœ¨è†å¬æ‚¨è¯´è¯...' : 'ç­‰å¾…æ‚¨è¯´è¯'}</span>
          </div>
          {isRecording && (
            <div className="vad-animation">
              <div className="vad-bar"></div>
              <div className="vad-bar"></div>
              <div className="vad-bar"></div>
              <div className="vad-bar"></div>
            </div>
          )}
        </>
      )}
    </div>
  )
}

export default VoiceActivityDetector

